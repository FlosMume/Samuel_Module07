{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9979e169-f27e-4303-9943-206e1294eef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "     ---------------------------------------- 0.0/721.7 kB ? eta -:--:--\n",
      "     -------------- ------------------------- 262.1/721.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 721.7/721.7 kB 1.9 MB/s  0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from gym) (2.1.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from gym) (3.0.0)\n",
      "Collecting gym_notices>=0.0.4 (from gym)\n",
      "  Downloading gym_notices-0.1.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Downloading gym_notices-0.1.0-py3-none-any.whl (3.3 kB)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml): started\n",
      "  Building wheel for gym (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827742 sha256=2f7b2f24f8682139579e46e806231d4397e9ac8b709643978ca298ee4136b714\n",
      "  Stored in directory: c:\\users\\ch939\\appdata\\local\\pip\\cache\\wheels\\1d\\34\\c6\\856a1e1eff47d8f18545c833b6138ae1e9f53c7de9bcc5f31d\n",
      "Successfully built gym\n",
      "Installing collected packages: gym_notices, gym\n",
      "\n",
      "   -------------------- ------------------- 1/2 [gym]\n",
      "   ---------------------------------------- 2/2 [gym]\n",
      "\n",
      "Successfully installed gym-0.26.2 gym_notices-0.1.0\n"
     ]
    }
   ],
   "source": [
    "! pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94cf8031-61ce-43b3-ad36-b0b8da69a083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: gym 0.26.2\n",
      "Uninstalling gym-0.26.2:\n",
      "  Successfully uninstalled gym-0.26.2\n",
      "Collecting gymnasium\n",
      "  Downloading gymnasium-1.2.0-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from gymnasium) (2.1.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from gymnasium) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\ch939\\anaconda3\\lib\\site-packages (from gymnasium) (4.12.2)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
      "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
      "Downloading gymnasium-1.2.0-py3-none-any.whl (944 kB)\n",
      "   ---------------------------------------- 0.0/944.3 kB ? eta -:--:--\n",
      "   ----------- ---------------------------- 262.1/944.3 kB ? eta -:--:--\n",
      "   --------------------------------- ------ 786.4/944.3 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 944.3/944.3 kB 2.1 MB/s  0:00:00\n",
      "Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Installing collected packages: farama-notifications, gymnasium\n",
      "\n",
      "   -------------------- ------------------- 1/2 [gymnasium]\n",
      "   ---------------------------------------- 2/2 [gymnasium]\n",
      "\n",
      "Successfully installed farama-notifications-0.0.4 gymnasium-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y gym && pip install gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd49d22-d642-4579-8ce0-c06da23fc29c",
   "metadata": {},
   "source": [
    "The Python library gym is a toolkit developed by OpenAI for building and experimenting with reinforcement learning (RL) environments. It provides a standardized API to interact with a wide variety of environments, making it easier to develop and compare RL algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644788e3-5efe-442a-8aea-5a1cb629b3a7",
   "metadata": {},
   "source": [
    "🔧 Key Features of gym:\n",
    "* Unified interface for different environments (e.g., games, robotics, control tasks).\n",
    "* Easy integration with popular RL libraries like Stable Baselines, RLlib, and TensorFlow/PyTorch.\n",
    "* Extensible: You can create custom environments.\n",
    "* Benchmarking: Includes classic control problems and Atari games for algorithm comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31105a1-e64a-4923-b771-2dce59e314e5",
   "metadata": {},
   "source": [
    "The gymnasium library is the actively maintained successor to OpenAI's original gym library, designed for developing and benchmarking reinforcement learning (RL) algorithms. It provides a standardized API and a rich set of environments for training RL agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9dd33e8-d491-4807-86a3-24dd646eab5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep   20 | R=  15.0 | avg(20)=  26.9 | best=  60.0\n",
      "Ep   40 | R=  17.0 | avg(20)=  38.4 | best=  76.0\n",
      "Ep   60 | R=  66.0 | avg(20)=  56.8 | best= 163.0\n",
      "Ep   80 | R=  83.0 | avg(20)=  55.5 | best= 163.0\n",
      "Ep  100 | R=  83.0 | avg(20)=  81.0 | best= 163.0\n",
      "Ep  120 | R= 177.0 | avg(20)= 100.8 | best= 200.0\n",
      "Ep  140 | R=  71.0 | avg(20)=  93.4 | best= 200.0\n",
      "Ep  160 | R=  41.0 | avg(20)= 106.9 | best= 200.0\n",
      "Ep  180 | R= 276.0 | avg(20)= 133.7 | best= 276.0\n",
      "Ep  200 | R= 467.0 | avg(20)= 184.7 | best= 467.0\n",
      "Ep  220 | R=  67.0 | avg(20)= 244.1 | best= 500.0\n",
      "Ep  240 | R= 165.0 | avg(20)= 150.2 | best= 500.0\n",
      "Ep  260 | R= 243.0 | avg(20)= 217.8 | best= 500.0\n",
      "Ep  280 | R= 378.0 | avg(20)= 309.6 | best= 500.0\n",
      "Ep  300 | R= 500.0 | avg(20)= 324.2 | best= 500.0\n",
      "Ep  320 | R= 126.0 | avg(20)= 306.9 | best= 500.0\n",
      "Ep  340 | R= 500.0 | avg(20)= 390.9 | best= 500.0\n",
      "Ep  360 | R= 343.0 | avg(20)= 346.8 | best= 500.0\n",
      "Ep  380 | R= 239.0 | avg(20)= 376.7 | best= 500.0\n",
      "Ep  400 | R= 109.0 | avg(20)= 235.3 | best= 500.0\n",
      "Ep  420 | R= 191.0 | avg(20)= 134.9 | best= 500.0\n",
      "Ep  440 | R=  98.0 | avg(20)= 169.5 | best= 500.0\n",
      "Ep  460 | R= 179.0 | avg(20)= 306.1 | best= 500.0\n",
      "Ep  480 | R= 156.0 | avg(20)= 162.1 | best= 500.0\n",
      "Ep  500 | R=  69.0 | avg(20)= 131.8 | best= 500.0\n",
      "Ep  520 | R= 114.0 | avg(20)= 139.6 | best= 500.0\n",
      "Ep  540 | R= 220.0 | avg(20)= 161.0 | best= 500.0\n",
      "Ep  560 | R= 500.0 | avg(20)= 398.1 | best= 500.0\n",
      "Ep  580 | R= 500.0 | avg(20)= 470.1 | best= 500.0\n",
      "Ep  600 | R= 500.0 | avg(20)= 488.1 | best= 500.0\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# REINFORCE (Vanilla Policy Gradient) — Minimal NumPy version\n",
    "# Task: OpenAI Gym CartPole-v1\n",
    "# Policy: linear + softmax  π(a|s) = softmax(W @ s)\n",
    "# Update rule: W ← W + α * G_t * ∇_W log π(a_t|s_t)\n",
    "# where G_t is the discounted return from time t.\n",
    "# ============================\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# --- Support both gymnasium and legacy gym ---\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "    NEW_API = True\n",
    "except ImportError:\n",
    "    import gym\n",
    "    NEW_API = False\n",
    "\n",
    "# ============ Hyperparameters ============\n",
    "ENV_NAME      = \"CartPole-v1\"\n",
    "GAMMA         = 0.99   # discount factor\n",
    "LR            = 0.02   # learning rate (small for stability with linear policy)\n",
    "NUM_EPISODES  = 600    # number of training episodes\n",
    "SEED          = 42     # RNG seed for reproducibility\n",
    "\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ============ Utilities ============\n",
    "\n",
    "def softmax(z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Numerically stable softmax:\n",
    "      softmax(z)_i = exp(z_i - max(z)) / sum_j exp(z_j - max(z))\n",
    "    \"\"\"\n",
    "    z = z - np.max(z)           # shift for numerical stability\n",
    "    e = np.exp(z)\n",
    "    return e / (np.sum(e) + 1e-8)\n",
    "\n",
    "def choose_action(W: np.ndarray, state: np.ndarray) -> (int, np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute action probabilities p = softmax(W @ s) and sample an action a ~ p.\n",
    "    Returns:\n",
    "      a: sampled action (int)\n",
    "      p: probability vector of shape (n_actions,)\n",
    "    \"\"\"\n",
    "    logits = W @ state                # shape: (n_actions,)\n",
    "    p = softmax(logits)\n",
    "    a = np.random.choice(len(p), p=p) # sample according to the stochastic policy\n",
    "    return a, p\n",
    "\n",
    "def discounted_returns(rewards, gamma=GAMMA):\n",
    "    \"\"\"\n",
    "    Compute discounted returns G_t for a single episode:\n",
    "      G_t = r_t + γ r_{t+1} + γ^2 r_{t+2} + ...\n",
    "    Implemented via a backward pass in O(T).\n",
    "    Also standardizes G to reduce variance (helps learning stability).\n",
    "    \"\"\"\n",
    "    G = np.zeros_like(rewards, dtype=np.float32)\n",
    "    running = 0.0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        running = rewards[t] + gamma * running\n",
    "        G[t] = running\n",
    "    # Standardize (optional but recommended for variance reduction)\n",
    "    if len(G) > 1:\n",
    "        G = (G - G.mean()) / (G.std() + 1e-8)\n",
    "    return G\n",
    "\n",
    "def grad_log_pi(s: np.ndarray, a: int, p: np.ndarray, n_actions: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    For a linear + softmax policy, the gradient of log π(a|s) w.r.t. W is:\n",
    "      ∇_W log π(a|s) = (one_hot(a) - p)[:, None] * s[None, :]\n",
    "    This is an outer product producing a matrix with shape (n_actions, n_features).\n",
    "    \"\"\"\n",
    "    one_hot = np.zeros(n_actions, dtype=np.float32)\n",
    "    one_hot[a] = 1.0\n",
    "    diff = one_hot - p                       # shape: (n_actions,)\n",
    "    return diff[:, None] * s[None, :]        # outer product -> (n_actions, n_features)\n",
    "\n",
    "# ============ Env & Parameters ============\n",
    "env = gym.make(ENV_NAME)\n",
    "# gymnasium reset returns (obs, info); legacy gym returns obs\n",
    "if NEW_API:\n",
    "    obs, _ = env.reset(seed=SEED)\n",
    "else:\n",
    "    obs = env.reset(seed=SEED)\n",
    "\n",
    "n_features = env.observation_space.shape[0]  # CartPole has 4-D state\n",
    "n_actions  = env.action_space.n              # CartPole has 2 actions\n",
    "\n",
    "# Initialize linear policy weights: shape (n_actions, n_features)\n",
    "W = np.random.randn(n_actions, n_features).astype(np.float32) * 0.01\n",
    "\n",
    "# ============ Training Loop ============\n",
    "best_reward = -np.inf\n",
    "reward_history = []\n",
    "\n",
    "for episode in range(1, NUM_EPISODES + 1):\n",
    "    # Collect one full episode (trajectory) before updating\n",
    "    states, actions, rewards, probs = [], [], [], []\n",
    "\n",
    "    if NEW_API:\n",
    "        s, _ = env.reset()\n",
    "    else:\n",
    "        s = env.reset()\n",
    "\n",
    "    done = False\n",
    "    ep_reward = 0.0\n",
    "\n",
    "    while not done:\n",
    "        s = np.asarray(s, dtype=np.float32)     # ensure NumPy 1-D array\n",
    "        a, p = choose_action(W, s)              # sample action from current policy\n",
    "\n",
    "        if NEW_API:\n",
    "            s_next, r, terminated, truncated, _ = env.step(a)\n",
    "            done = terminated or truncated\n",
    "        else:\n",
    "            s_next, r, done, _ = env.step(a)\n",
    "\n",
    "        # Store transition pieces\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        probs.append(p)\n",
    "\n",
    "        ep_reward += r\n",
    "        s = s_next\n",
    "\n",
    "    reward_history.append(ep_reward)\n",
    "    best_reward = max(best_reward, ep_reward)\n",
    "\n",
    "    # ----- REINFORCE update after the episode -----\n",
    "    G = discounted_returns(rewards, gamma=GAMMA)  # (optionally standardized)\n",
    "\n",
    "    # Accumulate policy gradients over the trajectory\n",
    "    grad_sum = np.zeros_like(W)\n",
    "    for s_t, a_t, p_t, G_t in zip(states, actions, probs, G):\n",
    "        grad = grad_log_pi(s_t, a_t, p_t, n_actions)  # (n_actions, n_features)\n",
    "        grad_sum += G_t * grad\n",
    "\n",
    "    # Gradient ASCENT (we maximize return)\n",
    "    W += LR * grad_sum\n",
    "\n",
    "    # ----- Logging -----\n",
    "    if episode % 20 == 0:\n",
    "        avg_last_20 = np.mean(reward_history[-20:])\n",
    "        print(f\"Ep {episode:4d} | R={ep_reward:6.1f} | avg(20)={avg_last_20:6.1f} | best={best_reward:6.1f}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "# After training, you should see the moving average reward go up\n",
    "# and often approach/clear the CartPole \"solved\" threshold.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71209c7d-7777-45b5-a06f-2af19462e3dd",
   "metadata": {},
   "source": [
    "What to notice\n",
    "\n",
    "* Policy: p(a|s) = softmax(W @ s) — a simple linear model is enough to learn CartPole.\n",
    "\n",
    "* Stochastic actions: sampled from p, ensuring exploration.\n",
    "\n",
    "* Returns: we compute discounted returns G_t for each time step; standardization reduces gradient variance.\n",
    "\n",
    "* Gradient: (one_hot(a) - p) ⊗ s (outer product) is all you need for the REINFORCE update.\n",
    "\n",
    "* Ascent vs. descent: we maximize expected return → do gradient ascent on W.\n",
    "\n",
    "Quick tuning tips\n",
    "\n",
    "* If learning is unstable: try smaller LR (e.g., 0.01 or 0.005) or increase NUM_EPISODES.\n",
    "\n",
    "* For even lower variance: subtract a baseline (e.g., episode mean return) from G_t, or move to Actor-Critic later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ed6532-f833-4148-8f2f-17431306aaa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
